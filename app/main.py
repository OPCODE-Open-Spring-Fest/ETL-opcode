import os

from app.etl.extract import extract
from app.etl.transform import transform
from app.etl.load import load

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
data_path = os.path.join(BASE_DIR, "data.csv")

def run_pipeline(csv_path: str =data_path, db_path: str = "etl_data.db"):
    """
    Run the complete ETL pipeline.
    
    Args:
        csv_path: Path to the input CSV file
        db_path: Path to the output SQLite database
    """
    try:
        print("ğŸš€ Starting ETL Pipeline")  # TODO (Find & Fix): Use logging instead of print
        print(f"ğŸ“ Input file: {csv_path}")
        print(f"ğŸ—„ï¸ Output database: {db_path}")
        print("-" * 50)
        
        # Extract
        print("ğŸ“¥ STEP 1: EXTRACT")
        df = extract(csv_path)
        print(f"âœ… Extracted {len(df)} rows")
        print(f"ğŸ“Š Columns: {list(df.columns)}")
        print()
        
        # Transform
        print("ğŸ”„ STEP 2: TRANSFORM")
        df_transformed = transform(df)
        print(f"âœ… Transformed data ready")
        print()
        
        # Load
        print("ğŸ“¤ STEP 3: LOAD")
        load(df_transformed, db_path)
        print()
        
        print("ğŸ‰ ETL Pipeline completed successfully!")
        print(f"ğŸ“ˆ Final dataset: {len(df_transformed)} rows, {len(df_transformed.columns)} columns")
        
    except FileNotFoundError as e:
        print(f"âŒ File Error: {e}")

    except ValueError as e:
        # TODO (Find & Fix): Error handling missing
        pass
    except Exception as e:
        # TODO (Find & Fix): Error handling missing
        pass

if __name__ == "__main__":    
    # Run the pipeline
    run_pipeline()